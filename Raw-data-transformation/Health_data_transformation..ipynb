{
  "metadata": {
    "name": "Health_data_transformation",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n# Read activity_environment_data.csv\r\nactivity_df \u003d spark.read.csv(\"hdfs:///user/hadoop/datasets/data/activity_environment_data.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\n\r\n# Read digital_interaction_data.csv\r\ndigital_df \u003d spark.read.csv(\"hdfs:///user/hadoop/datasets/data/digital_interaction_data.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\n\r\n# Read personal_health_data.csv\r\nhealth_df \u003d spark.read.csv(\"hdfs:///user/hadoop/datasets/data/personal_health_data.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\n\r\n# Display the first few rows to confirm\r\nactivity_df.show()\r\ndigital_df.show()\r\nhealth_df.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nactivity_df.describe().show()\r\ndigital_df.describe().show()\r\nhealth_df.describe().show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Drop rows with any missing value\nactivity_df_cleaned \u003d activity_df.na.drop()\ndigital_df_cleaned \u003d digital_df.na.drop()\nhealth_df_cleaned \u003d health_df.na.drop()\n\n# OR fill with default values (e.g., 0 for numerical columns)\n# activity_df_cleaned \u003d activity_df.na.fill(0)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nactivity_df_cleaned \u003d activity_df_cleaned.dropDuplicates()\ndigital_df_cleaned \u003d digital_df_cleaned.dropDuplicates()\nhealth_df_cleaned \u003d health_df_cleaned.dropDuplicates()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nactivity_df_cleaned.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nactivity_df_cleaned.columns\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndigital_df_cleaned.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nhealth_df_cleaned.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nactivity_df \u003d activity_df_cleaned.withColumnRenamed(\"Timestamp\", \"Activity_Timestamp\")\ndigital_df \u003d digital_df_cleaned.withColumnRenamed(\"Timestamp\", \"Digital_Timestamp\")\nhealth_df \u003d health_df_cleaned.withColumnRenamed(\"Timestamp\", \"Health_Timestamp\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\njoined_df1 \u003d activity_df.join(digital_df, [\"User_ID\"], \u0027outer\u0027)\nfinal_joined_df \u003d joined_df1.join(health_df, [\"User_ID\"], \u0027outer\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfinal_joined_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfinal_joined_df.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nspark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-west-1.amazonaws.com\")\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfinal_joined_df.write.csv(\"s3a://transformed-data-wearables/final_joined_data.csv\", header\u003dTrue, mode\u003d\"overwrite\")\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}